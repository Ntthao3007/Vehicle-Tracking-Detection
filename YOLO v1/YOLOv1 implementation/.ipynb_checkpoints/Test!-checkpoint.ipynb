{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "import json\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv1(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains the YOLOv1 model. It consists of 24 convolutional and\n",
    "    2 fully-connected layers which divide the input image into a \n",
    "    split_size x split_size grid and predict num_boxes bounding boxes per grid\n",
    "    cell. If the confidence of a bounding box reaches a certain value, it is \n",
    "    considered as a valid prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, split_size, num_boxes, num_classes):\n",
    "        \"\"\"\n",
    "        Initializes the neural-net with the parameter values to produce the\n",
    "        desired predictions.\n",
    "        \n",
    "        Parameters:\n",
    "            split_size (int): Size of the grid which is applied to the image.\n",
    "            num_boxes (int): Amount of bounding boxes which are predicted per \n",
    "            grid cell.\n",
    "            num_classes (int): Amount of different classes which are being \n",
    "            predicted by the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(YOLOv1, self).__init__()\n",
    "        self.darkNet = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, padding=3, stride=2, bias=False), # 3,448,448 -> 64,224,224\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(2, 2), # -> 64,112,112\n",
    "            \n",
    "            nn.Conv2d(64, 192, 3, padding=1, bias=False), # -> 192,112,112\n",
    "                nn.BatchNorm2d(192),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(2, 2), # -> 192,56,56\n",
    "            \n",
    "            nn.Conv2d(192, 128, 1, bias=False), # -> 192,56,56\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(128, 256, 3, padding=1, bias=False), # -> 256,56,56\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(256, 256, 1, bias=False), # -> 256,56,56\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(256, 512, 3, padding=1, bias=False), # -> 512,56,56\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(2, 2), # -> 512,28,28\n",
    "            \n",
    "            nn.Conv2d(512, 256, 1, bias=False), # -> 256,28,28\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(256, 512, 3, padding=1, bias=False), # -> 512,28,28\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(512, 256, 1, bias=False), # -> 256,28,28\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(256, 512, 3, padding=1, bias=False), # -> 512,28,28\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(512, 256, 1, bias=False), # -> 256,28,28\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(256, 512, 3, padding=1, bias=False), # -> 512,28,28\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(512, 256, 1, bias=False), # -> 256,28,28\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(256, 512, 3, padding=1, bias=False), # -> 512,28,28\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(512, 512, 1, bias=False), # -> 512,28,28\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(512, 1024, 3, padding=1, bias=False), # -> 1024,28,28\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(2, 2), # -> 1024,14,14\n",
    "            \n",
    "            nn.Conv2d(1024, 512, 1, bias=False), # -> 512,14,14\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(512, 1024, 3, padding=1, bias=False), # -> 1024,14,14\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(1024, 512, 1, bias=False), # -> 512,14,14\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(512, 1024, 3, padding=1, bias=False), # -> 1024,14,14\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1, bias=False), # -> 1024,14,14\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1, stride=2, bias=False), # -> 1024,7,7\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(1024, 1024, 3, padding=1, bias=False), # -> 1024,7,7\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1, bias=False), # -> 1024,7,7\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1024 * split_size * split_size, 4096),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Linear(4096, split_size * split_size * (num_classes + num_boxes*5)),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forwards the input tensor through the model to produce the predictions. \n",
    "        \n",
    "        Parameters:\n",
    "            x (tensor): A tensor of shape (batch_size, 3, 448, 448) which represents\n",
    "            a batch of input images.\n",
    "                \n",
    "        Returns:\n",
    "            x (tensor): A tensor of shape (batch_size, split_size * split_size * (num_classes\n",
    "            + num_boxes*5)) which contains the predicted bounding boxes.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.darkNet(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO_Loss():\n",
    "    \"\"\"\n",
    "    Used to calculate the loss for the YOLO-model using a batch of predictions and the corresponding\n",
    "    ground-truth labels. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, predictions, targets, split_size, num_boxes, num_classes, lambda_coord, lambda_noobj):\n",
    "        \"\"\"\n",
    "        Initialize the parameters for calculating the loss value.\n",
    "        \n",
    "        Parameters:\n",
    "            predictions (tensor): A tensor containing a mini-batch of training samples.\n",
    "            targets (tensor): A tensor containing a mini-batch of ground-truth labels.\n",
    "            split_size (int): Specifies the size of the grid which is applied to the image.\n",
    "            num_boxes (int): Amount of bounding boxes which are predicted by the YOLO-model.\n",
    "            num_classes (int): Amount of classes which are being predicted.\n",
    "            lambda_cooord (float): Hyperparameter for the loss regarding the bounding box coordinates.\n",
    "            lambda_noobj (float): Hyperparameter for the loss in case there is no object in the cell.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.predictions = predictions\n",
    "        self.targets = targets\n",
    "        self.split_size = split_size\n",
    "        self.cell_dim = int(448 / split_size) # Dimension of a single cell\n",
    "        self.num_boxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.final_loss = 0 # Here will the final value of the loss be stored\n",
    "        \n",
    "        \n",
    "    def loss(self):\n",
    "        \"\"\"\n",
    "        Main function for calculating the loss. Stores the calculated loss inside the final_loss atribute.\n",
    "        \"\"\"\n",
    "        \n",
    "        for sample in range(self.predictions.shape[0]):\n",
    "            mid_loss = 0 # Loss of the centre coordinates\n",
    "            dim_loss = 0 # Loss of the width and height values\n",
    "            conf_loss = 0 # Loss of the confidence score when there is an object in the cell\n",
    "            conf_loss_noobj = 0 # Loss of the confidence score when there is no object in the cell\n",
    "            class_loss = 0 # Loss of the class score\n",
    "            for cell_h in range(split_size):\n",
    "                for cell_w in range(split_size):\n",
    "                    # Check if the current cell contains an object\n",
    "                    if self.targets[sample, 0, cell_h, cell_w] != 1:\n",
    "                        conf_loss_noobj += self.noobj_loss(sample, cell_h, cell_w)\n",
    "                    else:\n",
    "                        mid_loss_local, dim_loss_local, conf_loss_local, class_loss_local = self.obj_loss(sample, cell_h, cell_w)\n",
    "                        mid_loss += mid_loss_local\n",
    "                        dim_loss += dim_loss_local\n",
    "                        conf_loss += conf_loss_local\n",
    "                        class_loss += class_loss_local\n",
    "                        \n",
    "            # Calculate the final loss by summing the other losses and applying the hyperparameters lambda_coord and lambda_noobj\n",
    "            self.final_loss += lambda_coord*mid_loss + lambda_coord*dim_loss + lambda_noobj*conf_loss_noobj + conf_loss + class_loss\n",
    "                        \n",
    "                    \n",
    "    def noobj_loss(self, sample, cell_h, cell_w):\n",
    "        \"\"\"\n",
    "        Calculates the loss value for a single cell in case there is no ground-truth object in that cell.\n",
    "        \n",
    "        Parameters:\n",
    "            sample (int): The index of the current sample from the batch.\n",
    "            cell_h (int): Index of the cell coordinate.\n",
    "            cell_w (int): Index of the cell coordinate.\n",
    "            \n",
    "        Return:\n",
    "            loss_value (float): The value of the loss with respect to the cell.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\" Version which only penalizes the best bounding box\n",
    "        best_box = 0 # Used to store the index of the best bounding box\n",
    "        max_conf = 0 # Used to store the highest bounding box confidence score\n",
    "        for box in range(self.num_boxes):\n",
    "            box_conf = self.predictions[sample, box*5, cell_h, cell_w]\n",
    "            if box_conf > max_conf:\n",
    "                max_conf = box_conf\n",
    "                best_box = box # Store the box index with the highest confidence\n",
    "        # Use the box with the highest confidence score for the conf_loss_noobj\n",
    "        loss_value = (0 - self.predictions[sample, best_box*5, cell_h, cell_w])**2\n",
    "        return loss_value\n",
    "        \"\"\"\n",
    "        \n",
    "        loss_value = 0\n",
    "        for box in range(self.num_boxes):\n",
    "            loss_value += (0 - self.predictions[sample, box*5, cell_h, cell_w])**2\n",
    "        \n",
    "        return loss_value\n",
    "\n",
    "        \n",
    "    def obj_loss(self, sample, cell_h, cell_w): \n",
    "        \"\"\"\n",
    "        Calculates the loss value for a single cell in case there is a ground-truth object in that cell.\n",
    "        \n",
    "        Parameters:\n",
    "            sample (int): The index of the current sample from the batch.\n",
    "            cell_h (int): Index of the cell coordinate.\n",
    "            cell_w (int): Index of the cell coordinate.\n",
    "            \n",
    "        Return:\n",
    "            mid_loss_local (float): Loss value for the mid coordinates of the bounding box.\n",
    "            dim_loss_local (float): Loss value for the height and width coordinates of the bounding box.\n",
    "            conf_loss_local (float): Loss value for the confidence score.\n",
    "            class_loss_local (float): Loss value for the class scores.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Finds the box with the highest IoU and stores its index in best_box\n",
    "        best_box = self.find_best_box(sample, cell_h, cell_w)\n",
    "        \n",
    "        # Calculates the loss for the centre coordinates\n",
    "        x_loss = (self.targets[sample, 1, cell_h, cell_w] - self.predictions[sample, 1+best_box*5, cell_h, cell_w])**2        \n",
    "        y_loss = (self.targets[sample, 2, cell_h, cell_w] - self.predictions[sample, 2+best_box*5, cell_h, cell_w])**2\n",
    "        mid_loss_local = x_loss + y_loss\n",
    "                \n",
    "        # Calculates the loss for the width and height values\n",
    "        w_loss = (math.sqrt(self.targets[sample, 3, cell_h, cell_w]) - math.sqrt(self.predictions[sample, 3+best_box*5, cell_h, cell_w]))**2\n",
    "        h_loss = (math.sqrt(self.targets[sample, 4, cell_h, cell_w]) - math.sqrt(self.predictions[sample, 4+best_box*5, cell_h, cell_w]))**2\n",
    "        dim_loss_local = w_loss + h_loss\n",
    "                \n",
    "        # Calculates the loss of the confidence score\n",
    "        conf_loss_local = (1 - self.predictions[sample, best_box*5, cell_h, cell_w])**2\n",
    "                \n",
    "        # Calculates the loss for the class scores\n",
    "        class_loss_local = 0\n",
    "        for c in range(self.num_classes):\n",
    "            class_loss_local += (self.targets[sample, 5+c, cell_h, cell_w] - self.predictions[sample, 5*num_boxes+c, cell_h, cell_w])**2\n",
    "            \n",
    "        return mid_loss_local, dim_loss_local, conf_loss_local, class_loss_local\n",
    "                        \n",
    "    \n",
    "    def find_best_box(self, sample, cell_h, cell_w):\n",
    "        \"\"\"\n",
    "        Finds the bounding box with the highest IoU with respect to the ground-truth bounding box.\n",
    "        \n",
    "        Parameters:\n",
    "            sample (int): The index of the current sample from the batch.\n",
    "            cell_h (int): Index of the cell coordinate.\n",
    "            cell_w (int): Index of the cell coordinate.\n",
    "            \n",
    "        Returns:\n",
    "            best_box (int): The index of the bounding box with the highest IoU.\n",
    "        \"\"\"\n",
    "        \n",
    "        best_box = 0\n",
    "        max_iou = 0\n",
    "        \n",
    "        # Transform the box coordinates into the corner format\n",
    "        t_box_coords = self.MidtoCorner(self.targets[sample, 1:5, cell_h, cell_w], cell_h, cell_w)\n",
    "        \n",
    "        for box in range(self.num_boxes):\n",
    "            # Transform the box coordinates into the corner format\n",
    "            p_box_coords = self.MidtoCorner(self.predictions[sample, 1+box*5:5+box*5, cell_h, cell_w], cell_h, cell_w)\n",
    "                    \n",
    "            box_score = self.IoU(t_box_coords, p_box_coords)\n",
    "            if box_score > max_iou:\n",
    "                max_iou = box_score\n",
    "                best_box = box # Store the box order with the highest IoU                \n",
    "        return best_box\n",
    "        \n",
    "        \n",
    "    def MidtoCorner(self, mid_box, cell_h, cell_w):\n",
    "        \"\"\"\n",
    "        Transforms bounding box coordinates which are in the mid YOLO format into the common corner format\n",
    "        with the correct pixel distance.\n",
    "        \n",
    "        Parameters:\n",
    "            mid_box (list): Bounding box coordinates which are in the mid YOLO format.\n",
    "            cell_h (int): Height index of the cell with the bounding box.\n",
    "            cell_w (int): Width index of the cell with the bounding box.\n",
    "            \n",
    "        Returns:\n",
    "            corner_box (list): A list containing the coordinates of the bounding box in the common\n",
    "            corner foormat\n",
    "        \"\"\"\n",
    "        \n",
    "        # Transform the coordinates from the YOLO format into normal pixel values\n",
    "        centre_x = mid_box[0]*self.cell_dim + self.cell_dim*cell_w\n",
    "        centre_y = mid_box[1]*self.cell_dim + self.cell_dim*cell_h\n",
    "        width = mid_box[2] * 448\n",
    "        height = mid_box[3] * 448\n",
    "\n",
    "        # Calculate the corner values of the bounding box\n",
    "        x1 = int(centre_x - width/2)\n",
    "        y1 = int(centre_y - height/2)\n",
    "        x2 = int(centre_x + width/2)\n",
    "        y2 = int(centre_y + height/2)\n",
    "\n",
    "        corner_box = [x1,y1,x2,y2]  \n",
    "        return corner_box    \n",
    "    \n",
    "    \n",
    "    def IoU(self, target, prediction):\n",
    "        \"\"\"\n",
    "        Calculates the Intersection over Union of two bounding boxes.\n",
    "        \n",
    "        Parameters:\n",
    "            target (list): A list with bounding box coordinates in the corner format.\n",
    "            predictions (list): A list with bounding box coordinates in the mid format.\n",
    "            \n",
    "        Returns:\n",
    "            iou_value (float): The score of the IoU over the two boxes\n",
    "        \"\"\"\n",
    "        # Calculate the corner coordinates of the intersection\n",
    "        i_x1 = max(target[0], prediction[0])\n",
    "        i_y1 = max(target[1], prediction[1])\n",
    "        i_x2 = min(target[2], prediction[2])\n",
    "        i_y2 = min(target[3], prediction[3])\n",
    "\n",
    "        intersection = max(0,(i_x2-i_x1)) * max(0,(i_y2-i_y1))    \n",
    "        union = ((target[2]-target[0]) * (target[3]-target[1])) + ((prediction[2]-prediction[0]) * (prediction[3]-prediction[1])) - intersection\n",
    "\n",
    "        iou_value = intersection / union    \n",
    "        return iou_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \"\"\"\n",
    "    This class uses its attributes to load the training data and transforms it into tensors.\n",
    "    The tensors are then stored in mini-batches inside the train_data list which is the final \n",
    "    product of this class. Multiple function calls of LoadData() will initialize the train_data \n",
    "    list with new tensors from the training data, excluding all the previous ones. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_files_path, target_files_path, category_list, split_size, batch_size, train_size):\n",
    "        \"\"\"\n",
    "        Initialize all parameters for loading and transforming the data into tensors.\n",
    "        \n",
    "        Parameters:\n",
    "            train_files_path (string): The path to the train image folder\n",
    "            target_files_path (string): The path to the json file containg the image labels\n",
    "            category_list (list): Reference list to all the label categories for object detection\n",
    "            split_size (int): Amount of grid cells\n",
    "            batch_size (int): Batch size\n",
    "            train_size (int): Amount of images which are loaded as training data \n",
    "        \"\"\"\n",
    "        \n",
    "        self.train_files_path = train_files_path\n",
    "        self.target_files_path = target_files_path       \n",
    "        self.category_list = category_list        \n",
    "        self.num_classes = len(category_list)       \n",
    "        self.cells = split_size        \n",
    "        self.batch_size = batch_size      \n",
    "        self.train_size = train_size\n",
    "        \n",
    "        self.train_files = [] # Will contain the remaining image names from the folder\n",
    "        self.target_files = [] # Will contain the json elements with the ground-truth labels\n",
    "        \n",
    "        self.train_data = [] # Will contain tuples with mini-batches of image and label tensors    \n",
    "        self.img_tensors = [] # Used to temporary store samples from a single batch\n",
    "        self.target_tensors = [] # Used to temporary store samples from a single batch\n",
    "        \n",
    "        # Define transform which is applied to every single image to resize and convert it into a tensor\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((448,448), Image.NEAREST),\n",
    "            transforms.ToTensor(),\n",
    "            ])\n",
    "    \n",
    "\n",
    "    def LoadFiles(self):\n",
    "        \"\"\"\n",
    "        First function to be executed.\n",
    "        Loads the images and the label file using the respective system path.\n",
    "        \"\"\"\n",
    "            \n",
    "        # All image names from the directory are loaded into the list train_files.\n",
    "        self.train_files = listdir(self.train_files_path)\n",
    "        \n",
    "        # The json file containing the labels is loaded into the list target_files.\n",
    "        f = open(self.target_files_path)\n",
    "        self.target_files = json.load(f)\n",
    "        \n",
    "        \n",
    "    def LoadData(self):\n",
    "        \"\"\"\n",
    "        Transforms the training images and labels into tensors and loads them into batches. Once a batch is\n",
    "        full, it is stored in the train_data list. Fills the train_data list with batches until the desired\n",
    "        train_size is reached. Every image that is loaded, is being excluded from future calls of this \n",
    "        function.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Reset the cache\n",
    "        self.train_data = []    \n",
    "        self.img_tensors = [] \n",
    "        self.target_tensors = [] \n",
    "\n",
    "        for i in range(len(self.train_files)):\n",
    "            # Check if batch is full and perhaps start a new one\n",
    "            if len(self.img_tensors) >= self.batch_size:\n",
    "                self.train_data.append((torch.stack(self.img_tensors), torch.stack(self.target_tensors)))\n",
    "                self.img_tensors = []\n",
    "                self.target_tensors = []\n",
    "                print('Loaded batch ', len(self.train_data), 'of ', int(self.train_size/self.batch_size))\n",
    "                print('Percentage Done: ', round(len(self.train_data)/int(self.train_size/self.batch_size)*100., 2), '%')\n",
    "                print('')\n",
    "            \n",
    "            if i == self.train_size:\n",
    "                break # The train_data list is full with the desired amount of batches\n",
    "                \n",
    "            # Extracts a single random image and the corresponding label, and transforms them into\n",
    "            # tensors. Both are appended to the img_tensors and target_tensors lists\n",
    "            self.extract_image_and_label() \n",
    "\n",
    "\n",
    "    def extract_image_and_label(self):\n",
    "        \"\"\"\n",
    "        Chooses a random image which is then being transformed into a tensor and stored.\n",
    "        Finds the corresponding label inside the json file which is then being transformed into a tensor\n",
    "        and stored. Stores both tensors inside the img_tensors and target_tensors lists.\n",
    "        \"\"\"\n",
    "        \n",
    "        img_tensor, chosen_image = self.extract_image()\n",
    "        target_tensor = self.extract_json_label(chosen_image)\n",
    "\n",
    "        self.img_tensors.append(img_tensor)\n",
    "        self.target_tensors.append(target_tensor)\n",
    "\n",
    "        \n",
    "    def extract_image(self):   \n",
    "        \"\"\"\n",
    "        Finds a random image from the train_files list and applies the transform to it. \n",
    " \n",
    "        Returns:\n",
    "            img_tensor (tensor): The tensor which contains the image values\n",
    "            f (string): The string name of the image file\n",
    "        \"\"\"    \n",
    "        \n",
    "        f = random.choice(self.train_files)\n",
    "        self.train_files.remove(f)\n",
    "        global img\n",
    "        img = Image.open(self.train_files_path + f)\n",
    "        img_tensor = self.transform(img) # Apply the transform to the image.\n",
    "        return img_tensor, f\n",
    "\n",
    "\n",
    "    def extract_json_label(self, chosen_image):\n",
    "        \"\"\"\n",
    "        Uses the name of the image to find the corresponding json element. Then it extracts the data and\n",
    "        transforms it into a tensor which is stored inside the target_tensors list.\n",
    "\n",
    "        Parameters:\n",
    "            chosen_image (string): The name of the image for which the label is needed.\n",
    "\n",
    "        Returns:\n",
    "            target_tensor (tensor): The tensor which contains the image labels\n",
    "        \"\"\"\n",
    "        \n",
    "        for json in self.target_files:\n",
    "            if json['name'] == chosen_image:\n",
    "                img_label = json\n",
    "                target_tensor = self.transform_label_to_tensor(img_label)\n",
    "                return target_tensor\n",
    "                #break\n",
    "\n",
    "        #target_tensor = self.transform_label_to_tensor(img_label)\n",
    "        #return target_tensor\n",
    "\n",
    "\n",
    "    def transform_label_to_tensor(self, img_label):\n",
    "        \"\"\"\n",
    "        Extracts the useful information from the json element and transforms them into a tensor.\n",
    "        \n",
    "        Parameters:\n",
    "            img_label (): A specific json element\n",
    "            \n",
    "        Returns:\n",
    "            target_tensor (tensor): A tensor of size (5+num_classes,cells,cells) which is used as the target of \n",
    "            the image.\n",
    "        \"\"\"\n",
    "        \n",
    "        target_tensor = torch.zeros(5+self.num_classes, self.cells, self.cells) # Here are the information stored\n",
    "\n",
    "        for labels in range(len(img_label[\"labels\"])):\n",
    "\n",
    "            # Store the category index if its contained within the category_list.\n",
    "            category = img_label[\"labels\"][labels][\"category\"]         \n",
    "            if category not in self.category_list:\n",
    "                continue\n",
    "            ctg_idx = self.category_list.index(category)\n",
    "\n",
    "            # Store the bounding box information and rescale it by the resize factor.\n",
    "            x1 = img_label[\"labels\"][labels][\"box2d\"][\"x1\"] * (448/img.size[0])\n",
    "            y1 = img_label[\"labels\"][labels][\"box2d\"][\"y1\"] * (448/img.size[1])\n",
    "            x2 = img_label[\"labels\"][labels][\"box2d\"][\"x2\"] * (448/img.size[0])\n",
    "            y2 = img_label[\"labels\"][labels][\"box2d\"][\"y2\"] * (448/img.size[1])\n",
    "\n",
    "            # Transforms the corner bounding box information into a mid bounding box information\n",
    "            x_mid = abs(x2 - x1) / 2 + x1\n",
    "            y_mid = abs(y2 - y1) / 2 + y1\n",
    "            width = abs(x2 - x1) \n",
    "            height = abs(y2 - y1) \n",
    "\n",
    "            # Size of a single cell\n",
    "            cell_dim = int(448 / self.cells)\n",
    "\n",
    "            # Determines the cell position of the bounding box\n",
    "            cell_pos_x = int(x_mid // cell_dim)\n",
    "            cell_pos_y = int(y_mid // cell_dim)\n",
    "\n",
    "            # Stores the information inside the target_tensor\n",
    "            if target_tensor[0][cell_pos_y][cell_pos_x] == 1: # Check if the cell already contains an object\n",
    "                continue\n",
    "            target_tensor[0][cell_pos_y][cell_pos_x] = 1\n",
    "            target_tensor[1][cell_pos_y][cell_pos_x] = (x_mid % cell_dim) / cell_dim\n",
    "            target_tensor[2][cell_pos_y][cell_pos_x] = (y_mid % cell_dim) / cell_dim\n",
    "            target_tensor[3][cell_pos_y][cell_pos_x] = width / 448\n",
    "            target_tensor[4][cell_pos_y][cell_pos_x] = height / 448\n",
    "            target_tensor[ctg_idx+5][cell_pos_y][cell_pos_x] = 1\n",
    "\n",
    "        return target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA IS BEING LOADED FOR A NEW EPOCH\n",
      "\n",
      "Saving checkpoint\n",
      "\n",
      "LOADING NEW BATCHES\n",
      "Remaining files:70000\n",
      "\n",
      "Loaded batch  1 of  10\n",
      "Percentage Done:  10.0 %\n",
      "\n",
      "Loaded batch  2 of  10\n",
      "Percentage Done:  20.0 %\n",
      "\n",
      "Loaded batch  3 of  10\n",
      "Percentage Done:  30.0 %\n",
      "\n",
      "Loaded batch  4 of  10\n",
      "Percentage Done:  40.0 %\n",
      "\n",
      "Loaded batch  5 of  10\n",
      "Percentage Done:  50.0 %\n",
      "\n",
      "Loaded batch  6 of  10\n",
      "Percentage Done:  60.0 %\n",
      "\n",
      "Loaded batch  7 of  10\n",
      "Percentage Done:  70.0 %\n",
      "\n",
      "Loaded batch  8 of  10\n",
      "Percentage Done:  80.0 %\n",
      "\n",
      "Loaded batch  9 of  10\n",
      "Percentage Done:  90.0 %\n",
      "\n",
      "Loaded batch  10 of  10\n",
      "Percentage Done:  100.0 %\n",
      "\n",
      "Train Epoch: 1 of 10 [Batch: 1/10 (10%)] Loss: 537.724243\n",
      "\n",
      "Train Epoch: 1 of 10 [Batch: 2/10 (20%)] Loss: 572.196594\n",
      "\n",
      "Train Epoch: 1 of 10 [Batch: 3/10 (30%)] Loss: 502.331604\n",
      "\n",
      "Train Epoch: 1 of 10 [Batch: 4/10 (40%)] Loss: 473.335388\n",
      "\n",
      "Train Epoch: 1 of 10 [Batch: 5/10 (50%)] Loss: 484.312164\n",
      "\n",
      "Train Epoch: 1 of 10 [Batch: 6/10 (60%)] Loss: 440.314667\n",
      "\n",
      "Train Epoch: 1 of 10 [Batch: 7/10 (70%)] Loss: 334.178284\n",
      "\n",
      "Train Epoch: 1 of 10 [Batch: 8/10 (80%)] Loss: 481.284241\n",
      "\n",
      "Train Epoch: 1 of 10 [Batch: 9/10 (90%)] Loss: 482.393860\n",
      "\n",
      "Train Epoch: 1 of 10 [Batch: 10/10 (100%)] Loss: 361.377563\n",
      "\n",
      "LOADING NEW BATCHES\n",
      "Remaining files:69900\n",
      "\n",
      "Loaded batch  1 of  10\n",
      "Percentage Done:  10.0 %\n",
      "\n",
      "Loaded batch  2 of  10\n",
      "Percentage Done:  20.0 %\n",
      "\n",
      "Loaded batch  3 of  10\n",
      "Percentage Done:  30.0 %\n",
      "\n",
      "Loaded batch  4 of  10\n",
      "Percentage Done:  40.0 %\n",
      "\n",
      "Loaded batch  5 of  10\n",
      "Percentage Done:  50.0 %\n",
      "\n",
      "Loaded batch  6 of  10\n",
      "Percentage Done:  60.0 %\n",
      "\n",
      "Loaded batch  7 of  10\n",
      "Percentage Done:  70.0 %\n",
      "\n",
      "Loaded batch  8 of  10\n",
      "Percentage Done:  80.0 %\n",
      "\n",
      "Loaded batch  9 of  10\n",
      "Percentage Done:  90.0 %\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 8 in argument 0, but got NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-741196fd006b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;31m# Start the training process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m \u001b[0mTrainNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-741196fd006b>\u001b[0m in \u001b[0;36mTrainNetwork\u001b[1;34m(num_epochs)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Remaining files:\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-c99c324115c7>\u001b[0m in \u001b[0;36mLoadData\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;31m# Check if batch is full and perhaps start a new one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_tensors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 8 in argument 0, but got NoneType"
     ]
    }
   ],
   "source": [
    "def TrainNetwork(num_epochs):\n",
    "    \"\"\"\n",
    "    Starts the training process of the model.\n",
    "    Parameters:\n",
    "        num_epochs (int): Amount of epochs for training the model\n",
    "    \"\"\"\n",
    "    \n",
    "    data = DataLoader(train_files_path, target_files_path, category_list, split_size, batch_size, train_size)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        print(\"DATA IS BEING LOADED FOR A NEW EPOCH\")\n",
    "        print(\"\")\n",
    "        data.LoadFiles()\n",
    "        \n",
    "        if len(data.train_files) % 1000 == 0:\n",
    "            print(\"Saving checkpoint\")\n",
    "            print(\"\")\n",
    "            torch.save(model.state_dict(), check_point_path)\n",
    "        \n",
    "        while len(data.train_files) > 0:\n",
    "            print(\"LOADING NEW BATCHES\")            \n",
    "            print(\"Remaining files:\" + str(len(data.train_files)))\n",
    "            print(\"\")\n",
    "            data.LoadData()\n",
    "            \n",
    "            for batch_idx, (train_data, target_data) in enumerate(data.train_data):\n",
    "                train_data = train_data.to(device)\n",
    "                target_data = target_data.to(device)\n",
    "    \n",
    "                predictions = model(train_data)\n",
    "                predictions = predictions.view(batch_size, num_boxes*5 + num_classes, split_size, split_size)\n",
    "                yolo_loss = YOLO_Loss(predictions, target_data, split_size, num_boxes, num_classes, lambda_coord, lambda_noobj)\n",
    "                yolo_loss.loss()\n",
    "                loss = yolo_loss.final_loss\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                print('Train Epoch: {} of {} [Batch: {}/{} ({:.0f}%)] Loss: {:.6f}'.format(\n",
    "                    epoch+1, num_epochs, batch_idx+1, len(data.train_data),\n",
    "                    (batch_idx+1) / len(data.train_data) * 100., loss))\n",
    "                print('')\n",
    "\n",
    "                \n",
    "# Dataset parameters\n",
    "train_files_path = \"C:/Users/alens/Desktop/Real-time-Object-Detection-for-Autonomous-Driving-using-Deep-Learning/YOLO v1/bdd100k/images/100k/train/\"\n",
    "target_files_path = \"C:/Users/alens/Desktop/Real-time-Object-Detection-for-Autonomous-Driving-using-Deep-Learning/YOLO v1/bdd100k_labels_release/bdd100k/labels/det_v2_train_release.json\"\n",
    "check_point_path = \"C:/Users/alens/Desktop/Real-time-Object-Detection-for-Autonomous-Driving-using-Deep-Learning/YOLO v1/YOLOv1 implementation/Training Checkpoint/checkpoint.pth\"\n",
    "category_list = [\"other vehicle\", \"pedestrian\", \"traffic light\", \"traffic sign\", \"truck\", \"train\", \"other person\", \"bus\", \"car\", \"rider\", \"motorcycle\", \"bicycle\", \"trailer\"]\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "split_size = 7\n",
    "num_boxes = 1\n",
    "num_classes = len(category_list)\n",
    "lambda_coord = 5\n",
    "lambda_noobj = 0.5\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "train_size = 100\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# Initialize model\n",
    "model = YOLOv1(split_size, num_boxes, num_classes).to(device)\n",
    "\n",
    "# Define the learning method as stochastic gradient descent\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "# Start the training process\n",
    "TrainNetwork(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
