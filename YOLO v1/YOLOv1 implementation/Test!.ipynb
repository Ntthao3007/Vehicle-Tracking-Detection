{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv1(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains the YOLOv1 model. It consists of 24 convolutional and\n",
    "    2 fully-connected layers which divide the input image into a \n",
    "    split_size x split_size grid and predict num_boxes bounding boxes per grid\n",
    "    cell. If the confidence of a bounding box reaches a certain value, it is \n",
    "    considered as a valid prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, split_size, num_boxes, num_classes):\n",
    "        \"\"\"\n",
    "        Initializes the neural-net with the parameter values to produce the\n",
    "        desired predictions.\n",
    "        \n",
    "        Parameters:\n",
    "            split_size (int): Size of the grid which is applied to the image.\n",
    "            num_boxes (int): Amount of bounding boxes which are predicted per \n",
    "                grid cell.\n",
    "            num_classes (int): Amount of different classes which are being \n",
    "                predicted by the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(YOLOv1, self).__init__()\n",
    "        self.darkNet = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, padding=3, stride=2, bias=False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(64, 192, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(192),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(192, 128, 1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(128, 256, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(256, 256, 1, bias=False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(256, 512, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(512, 256, 1, bias=False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(256, 512, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(512, 256, 1, bias=False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(256, 512, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(512, 256, 1, bias=False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(256, 512, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(512, 256, 1, bias=False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(256, 512, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(512, 512, 1, bias=False),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(512, 1024, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(1024, 512, 1, bias=False),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(512, 1024, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(1024, 512, 1, bias=False),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(512, 1024, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1, stride=2, bias=False),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Conv2d(1024, 1024, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.LeakyReLU(0.1),\n",
    "            )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * split_size * split_size, 4096),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(4096, split_size * split_size * (num_classes + num_boxes*5))\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forwards the input tensor through the model to produce the predictions. \n",
    "        Parameters:\n",
    "            x (tensor): A tensor of shape (batch_size, 3, 448, 448) which represents\n",
    "                a batch of images.\n",
    "        \"\"\"\n",
    "        x = self.darkNet(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \"\"\"\n",
    "    This class uses its attributes to load the training data and transforms it into tensors.\n",
    "    The tensors are then stored in mini-batches inside the train_data list which is the final \n",
    "    product of this class. Multiple function calls of LoadData() will initialize the train_data \n",
    "    list with new tensors from the training data, excluding all the previous ones. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_files_path, target_files_path, category_list, split_size, batch_size, train_size):\n",
    "        \"\"\"\n",
    "        Initialize all parameters for loading and transforming the data into tensors.\n",
    "        \n",
    "        Parameters:\n",
    "            train_files_path (string): The path to the train image folder\n",
    "            target_files_path (string): The path to the json file containg the image labels\n",
    "            category_list (list): Reference list to all the label categories for object detection\n",
    "            split_size (int): Amount of grid cells\n",
    "            batch_size (int): Batch size\n",
    "            train_size (int): Amount of images which are loaded as training data for one epoch\n",
    "        \"\"\"\n",
    "        \n",
    "        self.train_files_path = train_files_path\n",
    "        self.target_files_path = target_files_path       \n",
    "        self.category_list = category_list        \n",
    "        self.num_classes = len(category_list)       \n",
    "        self.cells = split_size        \n",
    "        self.batch_size = batch_size      \n",
    "        self.train_size = train_size\n",
    "        \n",
    "        self.train_files = [] # Will contain the remaining image names from the folder\n",
    "        self.target_files = [] # Will contain the json elements with the ground-truth labels\n",
    "        \n",
    "        self.train_data = [] # Will contain tuples with mini-batches of image and label tensors    \n",
    "        self.img_tensors = [] # Used to temporary store samples from a single batch\n",
    "        self.target_tensors = [] # Used to temporary store samples from a single batch\n",
    "        \n",
    "        # Define transform which is applied to every single image to resize and convert it into a tensor\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((448,448), Image.NEAREST),\n",
    "            transforms.ToTensor(),\n",
    "            ])\n",
    "    \n",
    "\n",
    "    def LoadFiles(self):\n",
    "        \"\"\"\n",
    "        First function to be executed.\n",
    "        Loads the images and the label file using the respective system path.\n",
    "        \"\"\"\n",
    "            \n",
    "        # All image names from the directory are loaded into the list train_files.\n",
    "        self.train_files = listdir(self.train_files_path)\n",
    "        \n",
    "        # The json file containing the labels is loaded into the list target_files.\n",
    "        f = open(self.target_files_path)\n",
    "        self.target_files = json.load(f)\n",
    "        \n",
    "        \n",
    "    def LoadData(self):\n",
    "        \"\"\"\n",
    "        Transforms the training images and labels into tensors and loads them into batches. Once a batch is\n",
    "        full, it is stored in the train_data list. Fills the train_data list with batches until the desired\n",
    "        train_size is reached. Every image that is loaded, is being excluded from future calls of this \n",
    "        function.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Reset the cache\n",
    "        self.train_data = []    \n",
    "        self.img_tensors = [] \n",
    "        self.target_tensors = [] \n",
    "\n",
    "        for i in range(len(self.train_files)):\n",
    "            # Check if batch is full and perhaps start a new one\n",
    "            if len(self.img_tensors) >= self.batch_size:\n",
    "                self.train_data.append((torch.stack(self.img_tensors), torch.stack(self.target_tensors)))\n",
    "                self.img_tensors = []\n",
    "                self.target_tensors = []\n",
    "                print('Loaded batch ', len(self.train_data), 'of ', int(self.train_size/self.batch_size))\n",
    "                print('Percentage Done: ', round(len(self.train_data)/int(self.train_size/self.batch_size)*100., 2), '%')\n",
    "                print('')\n",
    "            \n",
    "            if i == self.train_size:\n",
    "                break # The train_data list is full with the desired amount of batches\n",
    "                \n",
    "            # Extracts a single random image and the corresponding label, and transforms them into\n",
    "            # tensors. Both are appended to the img_tensors and target_tensors lists\n",
    "            self.extract_image_and_label() \n",
    "\n",
    "\n",
    "    def extract_image_and_label(self):\n",
    "        \"\"\"\n",
    "        Chooses a random image which is then being transformed into a tensor and stored.\n",
    "        Finds the corresponding label inside the json file which is then being transformed into a tensor\n",
    "        and stored. Stores both tensors inside the img_tensors and target_tensors lists.\n",
    "        \"\"\"\n",
    "        \n",
    "        img_tensor, chosen_image = self.extract_image()\n",
    "        target_tensor = self.extract_json_label(chosen_image)\n",
    "\n",
    "        self.img_tensors.append(img_tensor)\n",
    "        self.target_tensors.append(target_tensor)\n",
    "\n",
    "        \n",
    "    def extract_image(self):   \n",
    "        \"\"\"\n",
    "        Finds a random image from the train_files list and applies the transform to it. \n",
    " \n",
    "        Returns:\n",
    "            img_tensor (tensor): The tensor which contains the image values\n",
    "            f (string): The string name of the image file\n",
    "        \"\"\"    \n",
    "        \n",
    "        f = random.choice(self.train_files)\n",
    "        self.train_files.remove(f)\n",
    "        global img\n",
    "        img = Image.open(self.train_files_path + f)\n",
    "        img_tensor = self.transform(img) # Apply the transform to the image.\n",
    "        return img_tensor, f\n",
    "\n",
    "\n",
    "    def extract_json_label(self, chosen_image):\n",
    "        \"\"\"\n",
    "        Uses the name of the image to find the corresponding json element. Then it extracts the data and\n",
    "        transforms it into a tensor which is stored inside the target_tensors list.\n",
    "\n",
    "        Parameters:\n",
    "            chosen_image (string): The name of the image for which the label is needed.\n",
    "\n",
    "        Returns:\n",
    "            target_tensor (tensor): The tensor which contains the image labels\n",
    "        \"\"\"\n",
    "        \n",
    "        for json in self.target_files:\n",
    "            if json['name'] == chosen_image:\n",
    "                img_label = json\n",
    "                break\n",
    "\n",
    "        target_tensor = self.transform_label_to_tensor(img_label)\n",
    "        return target_tensor\n",
    "\n",
    "\n",
    "    def transform_label_to_tensor(self, img_label):\n",
    "        \"\"\"\n",
    "        Extracts the useful information from the json element and transforms them into a tensor.\n",
    "        \n",
    "        Parameters:\n",
    "            img_label (): A specific json element\n",
    "            \n",
    "        Returns:\n",
    "            target_tensor (tensor): A tensor of size (5+num_classes,cells,cells) which is used as the target of \n",
    "            the image.\n",
    "        \"\"\"\n",
    "        \n",
    "        target_tensor = torch.zeros(5+self.num_classes, self.cells, self.cells) # Here are the information stored\n",
    "\n",
    "        for labels in range(len(img_label[\"labels\"])):\n",
    "\n",
    "            # Store the category index if its contained within the category_list.\n",
    "            category = img_label[\"labels\"][labels][\"category\"]         \n",
    "            if category not in self.category_list:\n",
    "                continue\n",
    "            ctg_idx = self.category_list.index(category)\n",
    "\n",
    "            # Store the bounding box information and rescale it by the resize factor.\n",
    "            x1 = img_label[\"labels\"][labels][\"box2d\"][\"x1\"] * (448/img.size[0])\n",
    "            y1 = img_label[\"labels\"][labels][\"box2d\"][\"y1\"] * (448/img.size[1])\n",
    "            x2 = img_label[\"labels\"][labels][\"box2d\"][\"x2\"] * (448/img.size[0])\n",
    "            y2 = img_label[\"labels\"][labels][\"box2d\"][\"y2\"] * (448/img.size[1])\n",
    "\n",
    "            # Transforms the corner bounding box information into a mid bounding box information\n",
    "            x_mid = abs(x2 - x1) / 2 + x1\n",
    "            y_mid = abs(y2 - y1) / 2 + y1\n",
    "            width = abs(x2 - x1) \n",
    "            height = abs(y2 - y1) \n",
    "\n",
    "            # Size of a single cell\n",
    "            cell_dim = int(448 / self.cells)\n",
    "\n",
    "            # Determines the cell position of the bounding box\n",
    "            cell_pos_x = int(x_mid // cell_dim)\n",
    "            cell_pos_y = int(y_mid // cell_dim)\n",
    "\n",
    "            # Stores the information inside the target_tensor\n",
    "            if target_tensor[0][cell_pos_y][cell_pos_x] == 1: # Check if the cell already contains an object\n",
    "                continue\n",
    "            target_tensor[0][cell_pos_y][cell_pos_x] = 1\n",
    "            target_tensor[1][cell_pos_y][cell_pos_x] = (x_mid % cell_dim) / cell_dim\n",
    "            target_tensor[2][cell_pos_y][cell_pos_x] = (y_mid % cell_dim) / cell_dim\n",
    "            target_tensor[3][cell_pos_y][cell_pos_x] = width / 448\n",
    "            target_tensor[4][cell_pos_y][cell_pos_x] = height / 448\n",
    "            target_tensor[ctg_idx+5][cell_pos_y][cell_pos_x] = 1\n",
    "\n",
    "        return target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 784.00 MiB (GPU 0; 8.00 GiB total capacity; 6.11 GiB already allocated; 222.24 MiB free; 6.16 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c22eef5d1a60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;31m# Initialize model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mYOLOv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m# Define the learning method as stochastic gradient descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    608\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 784.00 MiB (GPU 0; 8.00 GiB total capacity; 6.11 GiB already allocated; 222.24 MiB free; 6.16 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "def TrainNetwork(num_epochs):\n",
    "    \"\"\"\n",
    "    Starts the training process of the model.\n",
    "    Parameters:\n",
    "        num_epochs (int): Amount of epochs for training the model\n",
    "    \"\"\"\n",
    "    \n",
    "    data = DataLoader(train_files_path, target_files_path, category_list, split_size, batch_size, train_size)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        print(\"DATA IS BEING LOADED FOR A NEW EPOCH\")\n",
    "        print(\"\")\n",
    "        data.LoadFiles()\n",
    "        \n",
    "        while len(data.train_files) > 0:\n",
    "            print(\"LOADING NEW BATCHES\")            \n",
    "            print(\"Remaining files:\" + str(len(data.train_files)))\n",
    "            print(\"\")\n",
    "            data.LoadData()\n",
    "            \n",
    "            for batch_idx, (train_data, target_data) in enumerate(data.train_data):\n",
    "                train_data = train_data.to(device)\n",
    "                target_data = target_data.to(device)\n",
    "    \n",
    "                predictions = model(train_data)\n",
    "                print(predictions.shape)\n",
    "                \"\"\"\n",
    "                loss = YOLO_Loss(predictions, target_data, split_size, num_boxes, num_classes, lambda_coord, lambda_noobj)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                print('Train Epoch: {} of {} [Batch: {}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, num_epochs, batch_idx, len(data.train_data),\n",
    "                    batch_idx / len(train_data) * 100., loss.data.item()))\n",
    "                \"\"\"\n",
    "                print('Train Epoch: {} of {} [Batch: {}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch+1, num_epochs, batch_idx+1, len(data.train_data),\n",
    "                    (batch_idx+1) / len(data.train_data) * 100., 10))\n",
    "                print('')\n",
    "\n",
    "# Dataset parameters\n",
    "train_files_path = \"C:/Users/alens/Desktop/Real-time-Object-Detection-for-Autonomous-Driving-using-Deep-Learning/YOLO v1/bdd100k/images/100k/val/\"\n",
    "target_files_path = \"C:/Users/alens/Desktop/Real-time-Object-Detection-for-Autonomous-Driving-using-Deep-Learning/YOLO v1/bdd100k_labels_release/bdd100k/labels/det_v2_val_release.json\"\n",
    "category_list = [\"other vehicle\", \"pedestrian\", \"traffic light\", \"traffic sign\", \"truck\", \"train\", \"other person\", \"bus\", \"car\", \"rider\", \"motorcycle\", \"bicycle\", \"trailer\"]\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "split_size = 7\n",
    "num_boxes = 2\n",
    "num_classes = len(category_list)\n",
    "lambda_coord = 5\n",
    "lambda_noobj = 5\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "train_size = 500\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "devie = 'cpu'\n",
    "\n",
    "# Initialize model\n",
    "model = YOLOv1(split_size, num_boxes, num_classes).to(device)\n",
    "\n",
    "# Define the learning method as stochastic gradient descent\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "# Start the training process\n",
    "TrainNetwork(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
